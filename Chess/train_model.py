import os
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import glob

# T·∫°o th∆∞ m·ª•c l∆∞u m√¥ h√¨nh n·∫øu ch∆∞a c√≥
os.makedirs("trained_models", exist_ok=True)

# ƒê·ªãnh nghƒ©a m√¥ h√¨nh m·∫°ng n∆°-ron ƒë∆°n gi·∫£n v·ªõi 2 l·ªõp Conv v√† 1 l·ªõp Dense
class ChessNet(nn.Module):
    def __init__(self):
        super(ChessNet, self).__init__()
        self.conv1 = nn.Conv2d(12, 64, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)
        self.fc1 = nn.Linear(64 * 8 * 8, 4096)  # M·ªói √¥ tr√™n b√†n c·ªù 64x64 = 4096 n∆∞·ªõc ƒëi
        self.softmax = nn.Softmax(dim=1)

    def forward(self, x):
        x = torch.relu(self.conv1(x))
        x = torch.relu(self.conv2(x))
        x = x.view(x.size(0), -1)  # flatten
        x = self.fc1(x)
        return self.softmax(x)

# Kh·ªüi t·∫°o model, loss function v√† optimizer
model = ChessNet()
optimizer = optim.Adam(model.parameters(), lr=0.001)
criterion = nn.CrossEntropyLoss()

# T√¨m t·∫•t c·∫£ c√°c batch d·ªØ li·ªáu
batch_files = glob.glob("train_models/chess_training_data_batch_*.npz")
batch_files.sort()  # s·∫Øp x·∫øp ƒë·ªÉ hu·∫•n luy·ªán theo th·ª© t·ª±

# Hu·∫•n luy·ªán m√¥ h√¨nh qua nhi·ªÅu epoch
epochs = 5
for epoch in range(epochs):
    print(f"\n Epoch {epoch + 1}/{epochs}")
    total_loss = 0
    for file in batch_files:
        print(f"   üîÑ ƒêang hu·∫•n luy·ªán v·ªõi: {file}")
        data = np.load(file)
        X = torch.tensor(data["X"], dtype=torch.float32).permute(0, 3, 1, 2)  # (B, 12, 8, 8)
        y = torch.tensor(np.argmax(data["y"], axis=1), dtype=torch.long)     # (B,)

        model.train()
        optimizer.zero_grad()
        outputs = model(X)
        loss = criterion(outputs, y)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()

    print(f" T·ªïng loss epoch {epoch + 1}: {total_loss:.4f}")

torch.save(model.state_dict(), "trained_models/chess_ai_model.pth")
print(" ƒê√£ l∆∞u m√¥ h√¨nh v√†o: trained_models/chess_ai_model.pth")
